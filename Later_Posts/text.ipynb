{
 "cells": [ 
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression as a Neural Network\n",
    "신경망을 구현시 기술\n",
    "1. m개의 traning set이 있으면 m개에 예제를 사용시 for문을 사용하지 않고 처리가 가능하지만\n",
    "신경망을 구현시에는 전체 traning set을 for문을 사용하지 않고 처리를 한다.\n",
    "2. 신경망을 구현할때 Forward Propagation, Back Propagation 계산을 진행한다.\n",
    "\n",
    "Logistic Regression은 신경망과 비슷한 형태를 띄기에 설명이 편리하다.\n",
    "\n",
    "입력 데이터에 따라서 분류하는 것은 Binary Classification 이진분류와 Mutliclass Classification 다중분류\n",
    "로 나누어진다. Logisitic Regression은 Binary Classification이므로 Binary Classification로 설명된다.\n",
    "\n",
    "고양이 데이터를 컴퓨터가 인지시, Binary Classification로서 고양이면 1, 아니면 0으로 Label을 출력하게 된다.\n",
    "고양이 데이터는 64px x 64px, RGB값으로 3개의 채널, 분리된 행렬이 합쳐진 형태이다. RGB의 값들을 하나의 feature vector x 에 나열하면 64 x 64 x 3의 차원으로 나타난다. 전체 차원은 12288차원이며, x의 차원을 의미하는 nx는 12288이다.\n",
    "\n",
    "표기법은 아래와 같다.\n",
    "(x,y) x = R^nx, y = {0,1} 이것은 한 개의 example이다.\n",
    "(블로그)\n",
    "\n",
    "[Logisitic Regression]\n",
    "Logisitic Regression는 다음과 같이 정의된다.\n",
    "Given x , (블로그)\n",
    "(주어진 input x 에 대해서 y가 1이 될 수 있는 확률을 구함)\n",
    "\n",
    "그리고 output의 파라미터는 w = Rnx 와 b = R 이므로\n",
    "yhat = (블로그) 로 나타낼 수 있다.\n",
    "\n",
    "하지만 Logisitic Regression 은 Binary Regression 에 효과가 뛰어나지는 않다. \n",
    "이진 분류는 0과 1로 구분되어야하는데, y의 값이 1보다 클 수도 있으며 -1 이하로 값이 도출될 수 있기 떄문이다.\n",
    "\n",
    "따라서 이를 해결하기 위해 sigmoid 함수를 이용하게 된다.\n",
    "(sigmoid 함수의 그래프)\n",
    "\n",
    "sigmoid 함수는 다음과 같다.\n",
    "(공식)\n",
    "\n",
    "따라서 Logisitic Regression 의 Output은 다음과 같이 사용된다.\n",
    "\n",
    "[Logisitic Regression Cost Function]\n",
    "\n",
    "가중치 w와 b 파라미터를 학습시키기 위해 Cost Function, 비용 함수를 정의한다.\n",
    "x^(i), y^(i), z^(i)로 정의가 되며, i는 traning example이다.\n",
    "\n",
    "Logisitic Regression 문제를 다음과 같이 정리하면\n",
    "(블로그)\n",
    "\n",
    "그리고 Loss(error) function으로\n",
    "(블로그) 를 사용할 수 있지만, 볼록한 부분이 있지 않기에 Local Optima 국소 최적 (파라미터가 국소 최적에 빠져 더이상 개선이 불가능함)\n",
    "적합하지 않다.\n",
    "따라서 볼록한 부분이 있는 Convex한 최적화 Loss function을 정의한다.\n",
    "\n",
    "(블로그)\n",
    "\n",
    "정의된 Loss function이 Binary Classification에 적합한 이유는\n",
    "(블로그)\n",
    "이기 때문이다.\n",
    "\n",
    "위에서 정의된 Loss function은 하나의 example에 대해서 정의되었기 때문에 비용함수를 정의하면 다음과 같다.\n",
    "\n",
    "(블로그)\n",
    "\n",
    "따라서 Cost Function은 파라미터에 대한 비용을 의미하게 된다. 그리고 모델을 학습하면서 파라미터 w,b를 찾고\n",
    "전체적인 학습 모델 비용 J를 줄이게 된다.\n",
    "\n",
    "[Gradient Descent]\n",
    "Cost Function은 비용 함수이기 때문에, 최소화 되어야 한다. Cost Function을 최소화시키는 w,b를 찾아야 하며\n",
    "Cost Function의 그래프는 아래와 같이 볼록한 형태를 보인다.\n",
    "w,b를 상수라 가정하여 J(w,b)가 최소화 하는 파라미터 w,b를 찾아야 한다.\n",
    "J함수 (Cost Function)은 여러 Local Optima가 있는 Non-convex가 아닌 convex함수에 있기 때문에 이러한 Cost funtion을 사용하게 되는 것이다.\n",
    "\n",
    "Gradient Descent 기울기 하강을 정리하자면\n",
    "1. w, b 를 초기화 한다. ( 0 혹은 랜덤 )\n",
    "2. 기울기가 가장 높은 방향으로 내려가며 최소값을 찾음\n",
    "\n",
    "2차원 그래프를 그리기 위해서 b 값을 생략하고 그리면 다음과 같다. \n",
    "Global Optima 전역 최솟값을 찾기 위해 Gradient Descent를 진행하게 된다.\n",
    "\n",
    "\n",
    "미분항이 dw라면 w:= w-adw로 나타낼 수 있다. 여기서 a는 Learning Rate (R) 이다.\n",
    "우측을 기준으로 Gradient Descent을 시작하면 미분항 dw는 그 지점으로 기울기 (R)이다. \n",
    "a값도 R, 미분항도 양수이기에 w의 값은 점점 감소하여 Global Optima로 향하게 된다.\n",
    "왼쪽을 기준으로 Gradient Descent을 시작하면 기울기는 음수로 지정되어 w는 증가한다.\n",
    "따라서 w가 증가함에 따라 Global Optima를 향하게 된다.\n",
    "\n",
    "* 미적분학 표기법에 따르면 한개의 변수에 대한 미분은 d, 두 개 이상의 변수는 d표시를 사용.\n",
    "\n",
    "[Derivatives]\n",
    "비전공자이고, 수학을 크게 잘하는 편이 아니였기 때문에 미분에 대해서 정리해봤습니다.\n",
    "미분은 크게 얘기를 하면, 함수 내의 기울기를 구하는 것으로 생각하면 쉽습니다.\n",
    "2차원 선형 그래프시, x값의 변화에 따른 y값의 변화는 기울기를 의미하는데\n",
    "만약 x축 a이 2일 경우 함수값에 의거하여 y값이 6이다.\n",
    "그렇다면 a 가 2.001로 증가했을 경우에는 y값은 6.003 로 증가하다면\n",
    "x값 증가 변화량 0.001이 y값 증가 변화량 0.003 의 변화가 일어났기에\n",
    "0.001 -> 0.003, 총 3배가 이루어졌음으로 확인 할 수 있다.\n",
    "이는 곧 기울기가 3이라는 뜻이고\n",
    "함수로 정의하자면\n",
    "(함수) = 3 으로 된다.\n",
    "이를 미분이라고 한다.\n",
    "\n",
    "[Computation Graph]\n",
    "Nerual Network은 계산을 Front Propagation(순전), Back Propagation(역전)으로 구성된다.\n",
    "계산 그래프를 그려보면 쉽게 알 수 있는다.\n",
    "\n",
    "손실 함수 J가 아래와 같이 예를 들면,\n",
    "J(a,b,c) = 3(a+bc)\n",
    "이 함수를 계산할 수 있는 아래와 같은 절차가 있다고 가정해보자.\n",
    "u = bc, v = a+u, J = 3v \n",
    "이 3가지 절차를 Computation Graph로 그리면 다음과 같다\n",
    "(그림)\n",
    "\n",
    "만약 a=5, b=3, c=2라면 u=6,v=11,j는 곧 33이 된다.\n",
    "\n",
    "특정 값에 대한 변수가 있는 경우 (여기선 u, v) 계산 그래프로 보면 쉽게 알 수 있다.\n",
    "현재의 계산은 Front Propagation 선전 계산이다.\n",
    "그렇다면 Back Propagation 역전 계산은 어떨까\n",
    "\n",
    "[Derivatives with a Computation Graph]\n",
    "(그림)\n",
    "선전 계산 값이 적혀진 Computation Graph이다.\n",
    "미분을 통해서 da,db,dc 값을 구해보자.\n",
    "역전 계산 절차는 선전 계산의 절차의 정 반대이기 때문에 J의 v에 대한 미분을 먼저 구해야한다.\n",
    "J는 v의 변화량에 따라 값이 변화되기 때문에\n",
    "J = 3v일 때,\n",
    "v = 11이 되고, v=11.001 이 되면 J=33.003이 된다. 따라서 dj/dv는 3이 된다.\n",
    "그 다음 a를 구하기 위해 dj/da를 구해야한다.\n",
    "v는 a의 변화량에 따라 값이 변화되기 때문에\n",
    "a = 5 -> 5.001\n",
    "v = 11 -> 11.001 이므로 a의 변화량은 v의 변화량을 1배 증가시키기 때문에 dv/da=1이며,\n",
    "j = 33 -> 33.003 이므로 a의 변화량은 j의 변화량을 3배 증가시키게 된다.\n",
    "따라서 dj/da=3 이 되는 것이다.\n",
    "\n",
    "a가 v에 영향을 주어 J까지 영향을 주는 것, Derivatives에서는 이런 상황을 chain rule(연쇄 법칙)이라고 정의한다. \n",
    "이를 한 문장으로 a값 변화가 v의 변화량 x v값 변화가 j의 변화량 이며\n",
    "아래와 같은 공식이 된다. \n",
    "\n",
    "실제 미분항들을 표기시 dvar라고 정의한다. dvar는 var값 변화시 최종 결과값 J의 변화량이며, dj/dvar을 의미한다.\n",
    "(var : variable, 변수이름) 따라서 dj/da 는 da. dj/dv는 dv이다.\n",
    "\n",
    "이와 같은 방법으로 du, db, dc를 구할 수 있다.\n",
    "\n",
    "[Logistic Regression Gradient Descent]\n",
    "Logisitic Regression에서 Gradient Descent를 적용하기 위해 미분시 방법이다.\n",
    "\n",
    "이전에 Logistic Regression을 통해서 다음과 같은 식을 보았다.\n",
    "(블로그)\n",
    "x1, x2 두가지의 feature만 존재한다고 가정했을때,\n",
    "Computation Graph는 아래와 같이 z, y^, L을 Front Propagation 절차로 계산할 것이다.\n",
    "\n",
    "Gradient Descent하기 위해 Back Propagation 절차로 계산하려면\n",
    "Loss 함수 L(a,y)를 a에 대해서 미분한 값 da를 구해야 한다.\n",
    "(detail 구하는 수식)\n",
    "다음 dz를 구하고 (detail 구하는 수식)\n",
    "마지막 단계에서는 w와 b에 대한 변화량을 게산해야 한다.\n",
    "dw1 즉 dl/dw1 와 dw2, db를 구하면 다음과 같다\n",
    "dw1 = x1dz dw2 = x2dz db=dz 이다.\n",
    "위 공식들을 사용해서 w와 b는 다음과 같이 업데이트가 된다.\n",
    "w1 := w1-adw1 ...\n",
    "\n",
    "[Gradient Desent of m example]\n",
    "모든 example에 대해서 Gradient Desent를 적용해야한다. 따라어 hat i 를 추가해서 다음과 같이 숙식을 정리하게 된다.\n",
    "그리고 각각의 training example에 대해 dw1^(i), dw2^(i), db^(i)를 구할 수 있다.\n",
    "(블로그)\n",
    "\n",
    "위 Cost function을 dw1으로 미분하면 다음과 같다.\n",
    "\n",
    "각 Example의 dw1을 구하고 평균치를 구하면, Gradient Descent에 바로 적용이 가능하다.\n",
    "\n",
    "Gradient Descent를 코드로 구현하면 아래와 같다\n",
    "\n",
    "현재는 feature가 2개이기지만, 앞으로 feature 개수가 많아지면 그 만큼 계산을 해주어야 한다.\n",
    "그리고 계산된 미분값들을 사용해 w,b가 업데이트 된다.\n",
    "\n",
    "딥러닝에서는 코드에 for 문이 있을 경우 비효율이 증가하기 때문에, 대규모 빅데이터 학습시 vectorization을 이용해 for문을 제거한다.\n",
    "\n",
    "* 왜 dL(a,y)/dz = a-y인지\n",
    "* vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shallow Neural Network]\n",
    "Forward Propagation과 Back Propagation을 사용하여 1개의 hidden layer, 은닞층을 가진 Neural Network을 설명한다.\n",
    "\n",
    "Logisitc Regression을 도식화하면 아래와 같다.\n",
    "(도식화 블로그)\n",
    "Logistic Regression의 Computation graph는 아래와 같이 도출된다.\n",
    "(도식화 블로그)\n",
    "\n",
    "따라 Cost Function L(a,y)를 구할 수 있고, Back Propagation을 통해 da, dz를 도출할 수 있다.\n",
    "da = ~(블로그)\n",
    "\n",
    "feature가 3개, hidden layer가 한개 있는 Neural Network 도식화는 다음과 같은데,\n",
    "(그림)\n",
    "이러한 Neural Network를 2-layer Neural Network 라고 하지만, feature를 의미하는 input layer, 연산 내 hidden layer,\n",
    "결과값 도출 output layer로 구성되어 있다. 하지만 feature layer는 고정값으로 포함이 되지 않는다.\n",
    "따라 hidden layer가 first layer이며, output layer가 last layer가 된다.\n",
    "\n",
    "위의 Neural Network 에서 Front Propagation, Back Propagation을 진행하면 아래와 같은데,\n",
    "(그림)\n",
    "input layer에서 hidden layer로 Front Propagation시, 기존 Logisitc Regression과 동일하다.\n",
    "*^[1]은 첫번째 레이어를 뜻한다.\n",
    "\n",
    "따라서 first layer에서 Front Propagation을 진행하면서 z^[1] = W^[1]x + b^[1]와 a = 오메가(z^[1])를 구하고, (오메가 = sigmoid function) First layer, hidden layer에서 구한 a^[1]를 사용하여 output layer의 계산 값들, z^[2] = W^[1]a^[1] + b^[2]와  a^[2] = 오메가(z^[2]))를 구할 수 있게 된다.\n",
    "last layer는 output layer으로 a^[2] = yhat 이다.\n",
    "\n",
    "이러한 Front Propagation 으로 Cost Function을 구하게되고, Back Propagation으로 hidden layer의 da^[2],dz^[2],dW^[2],db^[2]를 구하고 다시 da^[1], dz^[2], dw^[1], db^[1]을 구하는 것이 된다.\n",
    "\n",
    "[Neural Network Representation]\n",
    "Recap with Logisitc Regression에서 살핀 것 과 같이 아래는 3개의 층으로 이루어진 2-layer Neural Network이다.\n",
    "(Recap : input layer는 포함하지 않는다.)\n",
    "(그림)\n",
    "input layer를 x라 표현한다. input feature를 a^[0]으로 표기하기도 한다. \n",
    "*a = activation. 이 a는 다음 layer로 전달하는 값을 의미한다.\n",
    "그 다음 hidden layer에서 activation을 계산하게 되는데 이를 a^[1]로 표기한다. hidden layer 내의 activationd은 a_1^[1], a_2^[1] ~ 로 표기된다.\n",
    "마지막으로 output layer는 a^[2]가 계산되며, 실수값이 도출된다.\n",
    "\n",
    "한개의 training example 계산으로써, yhat = a^[2]가 된다.\n",
    "\n",
    "hidden layer의 파라미터는 w,b가 있고, hidden layer에 포함되어 있기에 w^[1], b^[1]로 표현이 된다. \n",
    "\n",
    "[Computing a Neural Network's Output]\n",
    "(메모 그림)\n",
    "Logisit Regression을 기준으로 다시 도식화하여 설명하면 아래와 같은 그림으로 도출된다.\n",
    "한 개의 traning example에 대해 hidden layer에서 가중치로 Z값을 계산하고, sigmoid function으로 activation을 계산하게 된다.\n",
    "*기본적으로 신경망은 이 작업을 훨씬 더 많이 수행한다.\n",
    "\n",
    "hidden layer의 첫번째와 두번째 activation node의 계산을 살펴보면,\n",
    "첫번째 activation노드에서 z^[1] = w_1^[1]^[T]x + b_1^[1]가 계산이 되고, 여기에 sigmoid function을 적용하여 a_^[1] = 오메가(z_1^[1])가 구하게 된다. 구해진 activation 은 a_i^[l]로 표기한다. *(l = layer, i = node number)\n",
    "같은 방식으로 2,3,4 node는 아래와 같다.\n",
    "(블로그)\n",
    "\n",
    "4개의 node를 구하기 위해선 for 문을 이용하여 구하게 되는데, 실제 신경망의 경우에는 수많은 node가 포함되기에 for문은 매우 비효율적이다.\n",
    "따라서 이 것들을 모두 Vectorization을 진행하게 된다.\n",
    "\n",
    "우선 z^[1]을 Vectorization을 진행 과정을 보자\n",
    "w을 모두 행렬에 쌓으면 다음과 같다.\n",
    "(블로그)\n",
    "Vectorization된 행렬을 사용하기에 대문자를 사용하고 레이어 넘버를 붙인다. W^[1]\n",
    "\n",
    "이 W 행렬은 (4,3) 차원이다. \n",
    "x는 3개의 feature로 구성된다.\n",
    "(블로그)\n",
    "나머지 b^[1]는 다음과 같이 (4x1)벡터로 나타내고, 이 것들을 사용해서 z^[1]을 구하면 아래와 같다.\n",
    "(블로그)\n",
    "\n",
    "z^[1]는 이렇게 구하고, 이를 통해 a^[1]을 구하면 다음과 같게 된다.\n",
    "(블로그)\n",
    "Output layer에서 위 과정을 반복하여 z^[2], a^[2]도 마찬가지로 구할 수 있다.\n",
    "\n",
    "w^[2] = (1,4), a^[1] = (4,1) b^[2] = (1,1)으로 볼 수 있다.\n",
    "\n",
    "[Vectorization across Multiple examples]\n",
    "위는 한 개의 training example에 대한 내용이다. 그렇다면 여러개의 examples를 이용하면 어떻게 될까\n",
    "m개의 training examples에 대해 모든 activation을 구해야 하기 때문에, 위의 내용을 m번 반복해야 한다.\n",
    "\n",
    "기존 for문으로 계산할수도 있지만, Vectorization을 사용하면 for문을 사용하지 않아도 된다.\n",
    "각 example에 대한 Vectorization 을 진행하면\n",
    "(블로그)\n",
    "여기서 모든 example에 대한 행렬의 표시로 대문자 X를 사용하고, (n_x, m) 행렬이다.\n",
    "\n",
    "이제 for문에서 계산해야할 수식은 다음과 같은데,\n",
    "(그림)\n",
    "X matrix와 같이 각 example을 column으로 쌓아 matrix을 구현하면 이와 같다.\n",
    "Z^[1], A^[1] (블로그)\n",
    "\n",
    "X,Z,A는 각 training example 들을 row방향으로 Indexing된것으로 볼 수 있다.\n",
    "X의 차원에서는 row = feature 개수 n, column = Traning example 개수 m이다.\n",
    "Z,A에서 row = 현재 layer의 activation node의 개수가 된다.\n",
    "즉, (Hidden layer units, Traning example)의 차원을 가진다.\n",
    "\n",
    "수식을 정리하면\n",
    "X와 A를 다음과 같이 traning example이 행렬의 열에 오도록 만들고,\n",
    "(블로그)\n",
    "for 문으로 작성된 부분을 Vectorization하여 계산하면 된다.\n",
    "(블로그)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Activation Funcrtion]\n",
    "Hidden layer에서 Actvation function을 사용하여 계산하는 것을 확인했는데,\n",
    "Sigmoid function이 아닌 다른 함수를 사용하는게 좋은 결과를 내기도 한다.\n",
    "Sigmoid function은 아래와 같다.\n",
    "(그래프)\n",
    "given(x) ~ 메모 \n",
    "이떄 오메가는 activation을 의미하고, 일반적으로 g^[1]로도 쓴다.\n",
    "\n",
    "Sigmoid function은 평균 1/2 값을 가지게 되는데, sigmoid보다 대부분 더 좋은 성능을 보이는\n",
    "tanh function, Hyperbolic tangent function이 있다.\n",
    "(그래프)\n",
    "tanh함수는 -1 부터 1사이의 값을 가지며, 수식은 다음과 같다.\n",
    "tanh(수식)\n",
    "tanh 함수는 데이터를 중심에 위치할 시 평균이 0인 데이터를 같게 된다.\n",
    "데이터를 중심에 위치함에 따라서 평균이 0이 되어 학습이 쉽게 된다.\n",
    "따라서 hidden layer에서는 sigmoid보다 tanh함수가 더욱 성능이 좋음을 알 수 있다.\n",
    "\n",
    "Binary Classification 에서 Output layer는 sigmoid가 사용된다.\n",
    "이유는 원하는 값 yhat이  0, 1 로 구성이 되어야 하는데 tanh function을 사용시 -1에서 1까지의 값을 도출하기 때문이다.\n",
    "\n",
    "sigmoid와 tanh function 의 단점은 z의 크기가 매우 커질때, Gradient(경도), 기울기 값이 매우 작은 값, 0에 가깝게 되어\n",
    "Gradient Descent 알고리즘 성능을 좋지 않게 만들게 된다.\n",
    "\n",
    "따라서 sigmoid, tanh function 이외의 function을 사용하게 되는데 그게 바로 ReLU Rectified Linear Unit이다.\n",
    "(그래프)\n",
    "z값이 양수일때는 Gradient값은 1, z가 음수이면 0이다. z가 0일때의 기울기는 구할 수 없지만, z가 0일때 grdient를 0 혹은 1로 취급하여 계산이 가능하기 때문에 사용이 된다 (수학적으로는 미분이 불가능)\n",
    "\n",
    "ReLU의 단점은 z가 음수일 경우에는 Gradient 값이 0이기에, 새로운 function인 Leaky Relu를 사용하기도 한다.\n",
    "(그래프)\n",
    "Leaky ReLU는 z가 음수일때 낮은 Gradient을 같게 된다. (사실 잘 사용하지 않음)\n",
    "\n",
    "ReLU와 Leaky ReLU는 sigmoid, tanh function에 비해 기울기에 의한 영향이 적다. (sigmoid, tanh은 기울기가 0인 지점이 존재함)\n",
    "ReLU function 또한 z값이 음수일때 기울기가 0이지만, 실제 hidden layer에서 대부분 z 값이 0보다 크기 때문에\n",
    "기울기에 의한 영향이 크게 미치지 않으며, 학습 속도가 빠르다.\n",
    "\n",
    "* Leaky ReLU는 0.01 파라미터를 사용하는 것이 보편화 되어있다. \n",
    "  \n",
    "[Why do we need non-linear activation function]\n",
    "그렇다면 왜 아러한 non-linear 비선형 function이 사용될까\n",
    "front Propagation 공식에 따라 살펴보자.\n",
    "(공식)\n",
    "g^[1](z^[1])를 z^[1]라고 변경하여 표시하고, linear activation function 선형함수 라고 가정해보자.\n",
    "그렇다면 a^[1], a^[2]는 그대로 z^[1], z^[2]가 되고, \n",
    "(공식)\n",
    "a^[2]는 결국 W'x + b' 가 된다.\n",
    "\n",
    "정리하면, y와 yhat은 linear function으로 계산이 되면, 결국 linear function 자체를 출력하게 된다.\n",
    "즉, 은닉층이 필요없게 되고 back Propagation을 하지 않아도 되어 결국 성능이 떨어지게 된다.\n",
    "\n",
    "linear activation function을 사용하는 경우는 g(z) = z 인 경우이다. y=R일 경우, 결과값이 R로 나와도 괜찮은 경우를 의미한다.\n",
    "(집값 예측 문제 등)\n",
    "이 경우에도 hidden layer를 사용시 linear actvation function을 사용해선 안된다.\n",
    "정리하자면 output layer 내에서만 linear activation function이 사용될 순 있어도, hidden layer만 사용하거나 output layer과 linear activation function을 동시에 사용할 수 없다. (-무한 <= yhat <= 무한)\n",
    "\n",
    "* output layer에 linear activation 사용시 hidden lyaer엔 non-linear activation function (sigmoid, tanh, ReLU, Leaky ReLU)사용\n",
    "\n",
    "[Actvation functions Derivatives]\n",
    "Back Propagation에서  Activation function을 미분하여 기울기가 어떻게 구해지는지 알아보자.\n",
    "(sigmoid)\n",
    "(tanh)\n",
    "(ReLU)\n",
    "(Leaky ReLU)\n",
    "\n",
    "[Gradient Descent for Neural Networks]\n",
    "Neural Networks에서 Hidden layer가 하나일 경우,\n",
    "parameter와 Cost Function은 다음과 간다.\n",
    "\n",
    "Parameters : \n",
    "Cost Function :\n",
    "\n",
    "여기서 yhat = a^[2]이며, 각 layer dimension을 계산하기 위하여 layer의 node 개수를 n_x = n^[0],n^[1],n^[2] = 1로 정의하게 된다. (binary Classification)\n",
    "(공식)\n",
    "\n",
    "grdient Descent는 다음과 같이 방식으로 진행된다.\n",
    "(수식)\n",
    "\n",
    "Binary Classification을 기준으로 forward Propagation, backward Propagation를 보자\n",
    "forward Propagation수식\n",
    "\n",
    "backward Propagation을 Python broadcasting을 이용하여 쓰면 아래와 같다.\n",
    "(블로그)\n",
    "\n",
    "\n",
    "[Random Initialization ]\n",
    "Neural Network에서 parameter W, Weight를 랜덤으로 초기화 하는 것이 중요해진다. logistic Regression 에서는 W를 0으로 초기화 하는것이 괜찮았지만, Neural Network에서 Weight를 0 으로 초기화 하고 Gradient Descent를 적용시 동작하지 않는다.\n",
    "\n",
    "(블로그)\n",
    "위 Neural Networks 에서 W^[1], b^[1], W^[2], b^[2]를 모두 0으로 초기화 했다고 가정해보자. (b = 0으로 초기화 해도 괜찮음)\n",
    "0으로 초기화 하고, hidden node를 구하면 hidden node가 똑같은 함수로 계산되어 a_1^[1] = a_2^[2]가 된다. 즉, 모든 hidden layer의 모든 activation fuctnion이 동일해져 같은 acitvation function으로만 계산이 되고 이는 곳 Output layer에 영향을 준다.\n",
    "따라서 W를 랜덤으로 초기화해 assymetric한 metrics를 만들어야 한다.\n",
    "(블로그)\n",
    "W^[1]는 np.random.randn((2,2))로 설정하면 가우시안 랜덤 변수로 생성되고, 이 숫자를 0.01과 같이 작은 값으로 곱한다. (다른 값도 가능. 아주 작은 무작위 값이 들어감. tanh, sigmoid function에서 weight가 매우 커지면, acitvation을 계산할때 z값이 매우 작거나 커지기 떄문에 Gradient Descent가 느려진다. 만약 Neural network 내에 sigmoid, tanh이 없으면 상관 없다.)\n",
    "현재는 Shallow Neural Network (2-layer neural network)이라 hidden uode가 많지 않아 0.01도 괜찮다.\n",
    "또한 parameter b의 경우 symmetric한 문제를 이끌어 내지 않기 때문에 0으로 초기화 해도 괜찮다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep multi-layer Nenural Network]\n",
    "Deep mutli-layer Nenural Network은, hidden layer가 2개 이상일 경우의 Neural Network을 의미한다.\n",
    "\n",
    "*Deep Neural Network 표기법\n",
    "4-layer Neural Network를 가정하면,\n",
    "L = 4 number of total layer....\n",
    "(블로그)\n",
    "\n",
    "input layer 는 0번째 layer 이므로, x는 a^[0]으로 나타낼 수 있다. output은 yhat으로 a^[L]으로 나타낸다.\n",
    "각 layer의 node는 아래와 같다.\n",
    "n^[1] = 5 , n^[2] = 5, n^[3] = 5, n^[4] = 1, n^[0] = 3 으로 볼 수 있다.\n",
    "\n",
    "[Forward Propagation]\n",
    "(블로그)\n",
    "위와 같은 Deep Nenural Network가 있을때, 한 개의 example에 대한 Forward Propagation 계산은 아래와 같다.\n",
    "우선 1,2,4 layer에서의 계산은 \n",
    "... 블로그\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notepad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a274fb57ecf12c6ae683a2840c33a91de3f68ee4f0be646129a08c12c88610b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
